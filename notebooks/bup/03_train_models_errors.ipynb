{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "b662632e",
            "metadata": {},
            "source": [
                "# 03. Train Models (Matches Script Structure)\n",
                "\n",
                "Matches the structure of 03_train_models.py with proper MLflow setup."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c7dd099b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# IMPORTS - Match script structure\n",
                "# ============================================================================\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import warnings\n",
                "import sys\n",
                "\n",
                "# Add project root to path\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "# Suppress annoying warnings\n",
                "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated\")\n",
                "\n",
                "# MLflow\n",
                "import mlflow\n",
                "import mlflow.sklearn\n",
                "import mlflow.tensorflow\n",
                "from mlflow.models import infer_signature\n",
                "\n",
                "# Scikit-learn\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "\n",
                "# ARIMA\n",
                "from pmdarima import auto_arima\n",
                "\n",
                "# TensorFlow\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
                "\n",
                "# Force CPU to avoid GPU issues\n",
                "try:\n",
                "    tf.config.set_visible_devices([], 'GPU')\n",
                "except:\n",
                "    pass\n",
                "\n",
                "# Project imports\n",
                "from utils.data_manager import DataManager\n",
                "\n",
                "# Constants - MATCH SCRIPT\n",
                "EXPERIMENT_NAME = \"EURUSD_Experiments\"\n",
                "\n",
                "# Setup visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (15, 7)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5dbac1e2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Evaluation Function - MATCH SCRIPT\n",
                "# ============================================================================\n",
                "\n",
                "def eval_metrics(actual, pred):\n",
                "    \"\"\"Match the exact function from 03_train_models.py\"\"\"\n",
                "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
                "    mae = mean_absolute_error(actual, pred)\n",
                "    \n",
                "    # Directional Accuracy\n",
                "    actual_arr = np.array(actual)\n",
                "    pred_arr = np.array(pred)\n",
                "    \n",
                "    actual_sign = np.sign(actual_arr)\n",
                "    pred_sign = np.sign(pred_arr)\n",
                "    accuracy = np.mean(actual_sign == pred_sign)\n",
                "    \n",
                "    return rmse, mae, accuracy\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e9aa308f",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "79bf2e40",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading data via DataManager...\")\n",
                "\n",
                "dm = DataManager(data_type='processed')\n",
                "train_df, test_df, scaler = dm.load_processed()\n",
                "\n",
                "if train_df is None:\n",
                "    print(\"Error: Could not load processed data. Run preprocessing first.\")\n",
                "\n",
                "print(f\"✅ Loaded {len(train_df)} train rows, {len(test_df)} test rows\")\n",
                "print(f\"Train: {train_df.index.min().date()} to {train_df.index.max().date()}\")\n",
                "print(f\"Test:  {test_df.index.min().date()} to {test_df.index.max().date()}\")\n",
                "\n",
                "# Get local path for scaler (for logging artifact) - MATCH SCRIPT\n",
                "scaler_path = dm.get_local_path('scaler.pkl')\n",
                "\n",
                "# Feature selection - MATCH SCRIPT STRUCTURE\n",
                "target_col = 'Target'\n",
                "\n",
                "# Feature subsets - MATCH SCRIPT\n",
                "FEATURES_LINREG = ['Return', 'MA_50', 'Return_20d', 'Lag_2', 'Lag_3']\n",
                "FEATURES_LSTM = ['Return', 'MA_50', 'Return_20d', 'Lag_2', 'Lag_3', 'Open', 'High', 'Low', 'Close']\n",
                "\n",
                "# Check which features exist\n",
                "linreg_cols = [c for c in FEATURES_LINREG if c in train_df.columns]\n",
                "lstm_cols = [c for c in FEATURES_LSTM if c in train_df.columns]\n",
                "\n",
                "print(f\"\\nLinear Regression features ({len(linreg_cols)}): {linreg_cols}\")\n",
                "print(f\"LSTM features ({len(lstm_cols)}): {lstm_cols}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a114afcb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# MLflow Setup - MATCH SCRIPT EXACTLY\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\nSetting up MLflow...\")\n",
                "\n",
                "# Check if running on AWS (simple check) - MATCH SCRIPT\n",
                "tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
                "if tracking_uri:\n",
                "    print(f\"Using Remote MLflow Tracking URI: {tracking_uri}\")\n",
                "    mlflow.set_tracking_uri(tracking_uri)\n",
                "else:\n",
                "    print(\"Using Local MLflow (sqlite:///mlflow.db)\")\n",
                "    mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
                "\n",
                "# Set experiment - MATCH SCRIPT\n",
                "mlflow.set_experiment(EXPERIMENT_NAME)\n",
                "\n",
                "# Common Tags - MATCH SCRIPT\n",
                "tags = {\n",
                "    \"developer\": \"User\",\n",
                "    \"project\": \"EURUSD_Capstone\",\n",
                "    \"data_version\": \"v1\"\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "38430be6",
            "metadata": {},
            "source": [
                "## 2. Train Ridge Regression (Improved Linear Regression)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eb65d398",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Model A: Ridge Regression (Improved Linear Regression)\n",
                "# ============================================================================\n",
                "\n",
                "with mlflow.start_run(run_name=\"Ridge_Regression_CV\") as run:\n",
                "    mlflow.set_tags(tags)\n",
                "    mlflow.set_tag(\"model_type\", \"RidgeRegression\")\n",
                "    \n",
                "    print(\"Training Ridge Regression with TimeSeries Cross-Validation...\")\n",
                "    \n",
                "    # Prepare data\n",
                "    X_train_lr = train_df[linreg_cols]\n",
                "    y_train_lr = train_df[target_col]\n",
                "    X_test_lr = test_df[linreg_cols]\n",
                "    y_test_lr = test_df[target_col]\n",
                "    \n",
                "    # Time Series Cross-Validation\n",
                "    tscv = TimeSeriesSplit(n_splits=5)\n",
                "    \n",
                "    # Hyperparameter grid\n",
                "    ridge = Ridge(random_state=42)\n",
                "    param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
                "    \n",
                "    # Grid search\n",
                "    grid_search = GridSearchCV(\n",
                "        ridge, \n",
                "        param_grid, \n",
                "        cv=tscv, \n",
                "        scoring='neg_mean_squared_error',\n",
                "        n_jobs=-1,\n",
                "        verbose=0\n",
                "    )\n",
                "    \n",
                "    grid_search.fit(X_train_lr, y_train_lr)\n",
                "    \n",
                "    # Best model\n",
                "    best_ridge = grid_search.best_estimator_\n",
                "    predictions = best_ridge.predict(X_test_lr)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    rmse, mae, da = eval_metrics(y_test_lr, predictions)\n",
                "    \n",
                "    print(f\"  Best alpha: {grid_search.best_params_['alpha']}\")\n",
                "    print(f\"  RMSE: {rmse}\")\n",
                "    print(f\"  MAE: {mae}\")\n",
                "    print(f\"  Directional Accuracy: {da}\")\n",
                "    \n",
                "    # Log Params - MATCH SCRIPT STRUCTURE\n",
                "    mlflow.log_params(best_ridge.get_params())\n",
                "    mlflow.log_param(\"features\", linreg_cols)\n",
                "    mlflow.log_param(\"best_alpha\", grid_search.best_params_['alpha'])\n",
                "    mlflow.log_param(\"cv_splits\", 5)\n",
                "    \n",
                "    # Log Metrics - MATCH SCRIPT\n",
                "    mlflow.log_metric(\"rmse\", rmse)\n",
                "    mlflow.log_metric(\"mae\", mae)\n",
                "    mlflow.log_metric(\"directional_accuracy\", da)\n",
                "    \n",
                "    # Log Scaler as Artifact - MATCH SCRIPT\n",
                "    mlflow.log_artifact(scaler_path, artifact_path=\"scaler\")\n",
                "    \n",
                "    # Log Feature Config - MATCH SCRIPT\n",
                "    feature_config = {\n",
                "        \"features\": linreg_cols,\n",
                "        \"target\": target_col,\n",
                "        \"model_type\": \"RidgeRegression\"\n",
                "    }\n",
                "    mlflow.log_dict(feature_config, \"feature_config.json\")\n",
                "    \n",
                "    # Infer and log signature - MATCH SCRIPT\n",
                "    signature = infer_signature(X_train_lr, predictions)\n",
                "    mlflow.sklearn.log_model(best_ridge, \"model\", signature=signature)\n",
                "    \n",
                "    ridge_results = {\n",
                "        'predictions': predictions,\n",
                "        'rmse': rmse,\n",
                "        'mae': mae,\n",
                "        'da': da,\n",
                "        'model': best_ridge\n",
                "    }\n",
                "    \n",
                "    print(\"✅ Ridge Regression trained and logged to MLflow\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "18d927e2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Ridge results\n",
                "plt.figure(figsize=(15, 5))\n",
                "\n",
                "plt.subplot(1, 3, 1)\n",
                "plt.scatter(y_test_lr.values, predictions, alpha=0.5)\n",
                "plt.plot([y_test_lr.min(), y_test_lr.max()], [y_test_lr.min(), y_test_lr.max()], \n",
                "         'r--', label='Perfect Prediction')\n",
                "plt.xlabel('Actual Returns')\n",
                "plt.ylabel('Predicted Returns')\n",
                "plt.title(f'Ridge Regression\\nDA: {da:.2%}')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 3, 2)\n",
                "# Feature importance\n",
                "feature_importance = pd.DataFrame({\n",
                "    'Feature': linreg_cols,\n",
                "    'Coefficient': best_ridge.coef_\n",
                "}).sort_values('Coefficient', key=abs, ascending=False)\n",
                "\n",
                "colors = ['green' if x > 0 else 'red' for x in feature_importance['Coefficient']]\n",
                "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors)\n",
                "plt.xlabel('Coefficient Value')\n",
                "plt.title(f'Feature Importance (alpha={grid_search.best_params_[\"alpha\"]})')\n",
                "plt.gca().invert_yaxis()\n",
                "\n",
                "plt.subplot(1, 3, 3)\n",
                "plt.plot(y_test_lr.values[:50], label='Actual', alpha=0.7)\n",
                "plt.plot(predictions[:50], label='Predicted', alpha=0.7)\n",
                "plt.xlabel('Test Days (First 50)')\n",
                "plt.ylabel('Return')\n",
                "plt.title('Ridge Predictions')\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "53a42708",
            "metadata": {},
            "source": [
                "## 3. Train ARIMA (Match Script Structure)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f551c89f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Model B: ARIMA - MATCH SCRIPT STRUCTURE\n",
                "# ============================================================================\n",
                "\n",
                "with mlflow.start_run(run_name=\"ARIMA\") as run:\n",
                "    mlflow.set_tags(tags)\n",
                "    mlflow.set_tag(\"model_type\", \"ARIMA\")\n",
                "    \n",
                "    print(\"Training ARIMA...\")\n",
                "    \n",
                "    # For ARIMA, use the 'Return_Unscaled' series - MATCH SCRIPT\n",
                "    train_series = train_df['Return_Unscaled']\n",
                "    test_series = test_df['Return_Unscaled'].values  # Iterator as in script\n",
                "    \n",
                "    # Auto ARIMA with improved settings\n",
                "    arima_model = auto_arima(\n",
                "        train_series, \n",
                "        seasonal=False, \n",
                "        trend='c',\n",
                "        start_p=1, start_q=1,\n",
                "        max_p=5, max_q=5,\n",
                "        d=None,\n",
                "        test='adf',\n",
                "        trace=True,\n",
                "        error_action='ignore',\n",
                "        suppress_warnings=True,\n",
                "        stepwise=True\n",
                "    )\n",
                "    \n",
                "    print(f\"  Best Order: {arima_model.order}\")\n",
                "    \n",
                "    # Log model immediately (Before updating) - MATCH SCRIPT\n",
                "    feature_config = {\n",
                "        \"features\": ['Return_Unscaled'],\n",
                "        \"target\": target_col,\n",
                "        \"model_type\": \"ARIMA\"\n",
                "    }\n",
                "    mlflow.log_dict(feature_config, \"feature_config.json\")\n",
                "    mlflow.sklearn.log_model(arima_model, \"model\")\n",
                "    \n",
                "    # Rolling Forecast - MATCH SCRIPT LOGIC\n",
                "    print(\"  Running Rolling Forecast for ARIMA...\")\n",
                "    \n",
                "    predictions = []\n",
                "    \n",
                "    for obs in test_series:\n",
                "        # Update with the new observation\n",
                "        arima_model.update(obs)\n",
                "        # Predict 1 step ahead\n",
                "        pred = arima_model.predict(n_periods=1)[0]\n",
                "        predictions.append(pred)\n",
                "        \n",
                "    predictions = np.array(predictions)\n",
                "    \n",
                "    # Align with test target\n",
                "    y_test_arima = test_df[target_col]\n",
                "    min_len = min(len(predictions), len(y_test_arima))\n",
                "    predictions = predictions[:min_len]\n",
                "    y_test_arima = y_test_arima.values[:min_len]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    rmse, mae, da = eval_metrics(y_test_arima, predictions)\n",
                "    \n",
                "    print(f\"  RMSE: {rmse}\")\n",
                "    print(f\"  MAE: {mae}\")\n",
                "    print(f\"  Directional Accuracy: {da}\")\n",
                "    \n",
                "    # Log parameters - MATCH SCRIPT\n",
                "    mlflow.log_param(\"order\", str(arima_model.order))\n",
                "    mlflow.log_param(\"seasonal_order\", str(arima_model.seasonal_order))\n",
                "    mlflow.log_param(\"aic\", arima_model.aic())\n",
                "    \n",
                "    # Log metrics - MATCH SCRIPT\n",
                "    mlflow.log_metric(\"rmse\", rmse)\n",
                "    mlflow.log_metric(\"mae\", mae)\n",
                "    mlflow.log_metric(\"directional_accuracy\", da)\n",
                "    \n",
                "    arima_results = {\n",
                "        'predictions': predictions,\n",
                "        'rmse': rmse,\n",
                "        'mae': mae,\n",
                "        'da': da,\n",
                "        'model': arima_model\n",
                "    }\n",
                "    \n",
                "    print(\"✅ ARIMA trained and logged to MLflow\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a0b5438",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize ARIMA results\n",
                "plt.figure(figsize=(15, 5))\n",
                "\n",
                "plt.subplot(1, 3, 1)\n",
                "plt.scatter(y_test_arima, predictions, alpha=0.5)\n",
                "plt.plot([y_test_arima.min(), y_test_arima.max()], [y_test_arima.min(), y_test_arima.max()], \n",
                "         'r--', label='Perfect Prediction')\n",
                "plt.xlabel('Actual Returns')\n",
                "plt.ylabel('Predicted Returns')\n",
                "plt.title(f'ARIMA {arima_model.order}\\nDA: {da:.2%}')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 3, 2)\n",
                "plt.plot(y_test_arima[:50], label='Actual', alpha=0.7)\n",
                "plt.plot(predictions[:50], label='Predicted', alpha=0.7)\n",
                "plt.xlabel('Test Days (First 50)')\n",
                "plt.ylabel('Return')\n",
                "plt.title('ARIMA Predictions')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 3, 3)\n",
                "plt.plot(predictions, alpha=0.7)\n",
                "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
                "plt.xlabel('Test Days')\n",
                "plt.ylabel('Predicted Return')\n",
                "plt.title('ARIMA All Predictions')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b9390cf2",
            "metadata": {},
            "source": [
                "## 4. Train LSTM (Match Script Structure)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e46fe29d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Model C: LSTM - MATCH SCRIPT STRUCTURE\n",
                "# ============================================================================\n",
                "\n",
                "with mlflow.start_run(run_name=\"LSTM\") as run:\n",
                "    mlflow.set_tags(tags)\n",
                "    mlflow.set_tag(\"model_type\", \"LSTM\")\n",
                "    \n",
                "    print(\"Training LSTM...\")\n",
                "    \n",
                "    # Prepare data\n",
                "    X_train_lstm = train_df[lstm_cols]\n",
                "    y_train_lstm = train_df[target_col]\n",
                "    X_test_lstm = test_df[lstm_cols]\n",
                "    y_test_lstm = test_df[target_col]\n",
                "    \n",
                "    # Helper to create sequences - MATCH SCRIPT\n",
                "    def create_sequences(data, target, time_steps):\n",
                "        X, y = [], []\n",
                "        for i in range(len(data) - time_steps + 1):\n",
                "            X.append(data[i:(i + time_steps)])\n",
                "            val = target[i + time_steps - 1]\n",
                "            y.append(val)\n",
                "        return np.array(X), np.array(y)\n",
                "    \n",
                "    # Config - MATCH SCRIPT\n",
                "    time_steps = 60\n",
                "    n_features = len(lstm_cols)\n",
                "    \n",
                "    # Generate Sequences\n",
                "    X_train_seq, y_train_seq = create_sequences(X_train_lstm.values, y_train_lstm.values, time_steps)\n",
                "    X_test_seq, y_test_seq = create_sequences(X_test_lstm.values, y_test_lstm.values, time_steps)\n",
                "    \n",
                "    print(f\"  LSTM Input Shape: {X_train_seq.shape}\")\n",
                "    \n",
                "    epochs = 20\n",
                "    batch_size = 32\n",
                "    \n",
                "    # Build model - Simplified to avoid training issues\n",
                "    model = Sequential()\n",
                "    model.add(Input(shape=(time_steps, n_features)))\n",
                "    \n",
                "    # Layer 1\n",
                "    model.add(LSTM(100, return_sequences=True))\n",
                "    model.add(Dropout(0.2))\n",
                "    \n",
                "    # Layer 2\n",
                "    model.add(LSTM(100, return_sequences=True))\n",
                "    model.add(Dropout(0.2))\n",
                "    \n",
                "    # Layer 3\n",
                "    model.add(LSTM(50, return_sequences=True))\n",
                "    model.add(Dropout(0.2))\n",
                "    \n",
                "    # Layer 4\n",
                "    model.add(LSTM(50, return_sequences=False))\n",
                "    model.add(Dropout(0.2))\n",
                "    \n",
                "    # Output\n",
                "    model.add(Dense(25))\n",
                "    model.add(Dense(1))\n",
                "    \n",
                "    model.compile(optimizer='adam', loss='mse')\n",
                "    \n",
                "    # Train\n",
                "    print(f\"  Training for {epochs} epochs...\")\n",
                "    history = model.fit(\n",
                "        X_train_seq, y_train_seq, \n",
                "        epochs=epochs, \n",
                "        batch_size=batch_size, \n",
                "        verbose=1,\n",
                "        validation_split=0.1\n",
                "    )\n",
                "    \n",
                "    # Predict\n",
                "    predictions = model.predict(X_test_seq)\n",
                "    predictions = predictions.flatten()\n",
                "    \n",
                "    # Calculate metrics\n",
                "    rmse, mae, da = eval_metrics(y_test_seq, predictions)\n",
                "    \n",
                "    print(f\"  RMSE: {rmse}\")\n",
                "    print(f\"  MAE: {mae}\")\n",
                "    print(f\"  Directional Accuracy: {da}\")\n",
                "    \n",
                "    # Log Architecture Details - MATCH SCRIPT STRUCTURE\n",
                "    mlflow.log_param(\"input_shape\", f\"[{X_train_seq.shape[0]}, {time_steps}, {n_features}]\")\n",
                "    mlflow.log_param(\"time_steps\", time_steps)\n",
                "    mlflow.log_param(\"n_features\", n_features)\n",
                "    mlflow.log_param(\"epochs\", epochs)\n",
                "    mlflow.log_param(\"batch_size\", batch_size)\n",
                "    \n",
                "    # Log Layer Details\n",
                "    model_summary = []\n",
                "    for i, layer in enumerate(model.layers):\n",
                "        layer_config = layer.get_config()\n",
                "        layer_name = layer_config['name']\n",
                "        layer_class = layer.__class__.__name__\n",
                "        units = layer_config.get('units', 'N/A')\n",
                "        activation = layer_config.get('activation', 'N/A')\n",
                "        \n",
                "        mlflow.log_param(f\"layer_{i}_class\", layer_class)\n",
                "        mlflow.log_param(f\"layer_{i}_units\", units)\n",
                "        mlflow.log_param(f\"layer_{i}_activation\", activation)\n",
                "        \n",
                "        model_summary.append(f\"{layer_class}(units={units}, activation={activation})\")\n",
                "        \n",
                "    mlflow.log_param(\"model_arch_summary\", \" -> \".join(model_summary))\n",
                "    \n",
                "    # Log metrics - MATCH SCRIPT\n",
                "    mlflow.log_metric(\"rmse\", rmse)\n",
                "    mlflow.log_metric(\"mae\", mae)\n",
                "    mlflow.log_metric(\"directional_accuracy\", da)\n",
                "    \n",
                "    # Log Scaler as Artifact - MATCH SCRIPT\n",
                "    mlflow.log_artifact(scaler_path, artifact_path=\"scaler\")\n",
                "    \n",
                "    # Log Feature Config\n",
                "    feature_config = {\n",
                "        \"features\": lstm_cols,\n",
                "        \"target\": target_col,\n",
                "        \"time_steps\": time_steps,\n",
                "        \"n_features\": n_features,\n",
                "        \"model_type\": \"LSTM\"\n",
                "    }\n",
                "    mlflow.log_dict(feature_config, \"feature_config.json\")\n",
                "\n",
                "    # Infer and log signature - MATCH SCRIPT\n",
                "    signature = infer_signature(X_train_seq, predictions)\n",
                "    mlflow.tensorflow.log_model(model, \"model\", signature=signature)\n",
                "    \n",
                "    lstm_results = {\n",
                "        'predictions': predictions,\n",
                "        'rmse': rmse,\n",
                "        'mae': mae,\n",
                "        'da': da,\n",
                "        'model': model\n",
                "    }\n",
                "    \n",
                "    print(\"✅ LSTM trained and logged to MLflow\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e2848773",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize LSTM results\n",
                "plt.figure(figsize=(15, 5))\n",
                "\n",
                "plt.subplot(1, 3, 1)\n",
                "plt.plot(history.history['loss'], label='Train Loss')\n",
                "plt.plot(history.history['val_loss'], label='Val Loss')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('MSE Loss')\n",
                "plt.title('LSTM Training History')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 3, 2)\n",
                "plt.scatter(y_test_seq, predictions, alpha=0.5)\n",
                "plt.plot([y_test_seq.min(), y_test_seq.max()], [y_test_seq.min(), y_test_seq.max()], \n",
                "         'r--', label='Perfect Prediction')\n",
                "plt.xlabel('Actual Returns')\n",
                "plt.ylabel('Predicted Returns')\n",
                "plt.title(f'LSTM Predictions\\nDA: {da:.2%}')\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 3, 3)\n",
                "plt.plot(y_test_seq[:50], label='Actual', alpha=0.7)\n",
                "plt.plot(predictions[:50], label='Predicted', alpha=0.7)\n",
                "plt.xlabel('Test Days (First 50)')\n",
                "plt.ylabel('Return')\n",
                "plt.title('LSTM Predictions')\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "77a88a95",
            "metadata": {},
            "source": [
                "## 5. Model Comparison (Visual Only)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "55efc55f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# Model Comparison Visualization\n",
                "# ============================================================================\n",
                "\n",
                "# Collect all results\n",
                "all_results = {\n",
                "    'Ridge': ridge_results,\n",
                "    'ARIMA': arima_results,\n",
                "    'LSTM': lstm_results\n",
                "}\n",
                "\n",
                "# Create comparison dataframe\n",
                "comparison_data = []\n",
                "for model_name, results in all_results.items():\n",
                "    comparison_data.append({\n",
                "        'Model': model_name,\n",
                "        'RMSE': results['rmse'],\n",
                "        'MAE': results['mae'],\n",
                "        'Directional_Accuracy': results['da']\n",
                "    })\n",
                "\n",
                "comparison_df = pd.DataFrame(comparison_data).set_index('Model')\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"MODEL COMPARISON\")\n",
                "print(\"=\"*80)\n",
                "print(comparison_df)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5b95bf38",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visual comparison\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
                "\n",
                "# 1. Directional Accuracy\n",
                "models = list(all_results.keys())\n",
                "da_values = [all_results[m]['da'] for m in models]\n",
                "\n",
                "axes[0, 0].bar(models, da_values, color=['blue', 'orange', 'green'])\n",
                "axes[0, 0].axhline(y=0.5, color='red', linestyle='--', label='Random (50%)')\n",
                "axes[0, 0].set_title('Directional Accuracy (Higher is Better)')\n",
                "axes[0, 0].set_ylabel('Accuracy')\n",
                "axes[0, 0].set_ylim([0, 0.6])\n",
                "for i, v in enumerate(da_values):\n",
                "    axes[0, 0].text(i, v + 0.01, f'{v:.2%}', ha='center')\n",
                "\n",
                "# 2. RMSE\n",
                "rmse_values = [all_results[m]['rmse'] for m in models]\n",
                "axes[0, 1].bar(models, rmse_values, color=['blue', 'orange', 'green'])\n",
                "axes[0, 1].set_title('RMSE (Lower is Better)')\n",
                "axes[0, 1].set_ylabel('RMSE')\n",
                "for i, v in enumerate(rmse_values):\n",
                "    axes[0, 1].text(i, v + 0.0001, f'{v:.6f}', ha='center')\n",
                "\n",
                "# 3. MAE\n",
                "mae_values = [all_results[m]['mae'] for m in models]\n",
                "axes[0, 2].bar(models, mae_values, color=['blue', 'orange', 'green'])\n",
                "axes[0, 2].set_title('MAE (Lower is Better)')\n",
                "axes[0, 2].set_ylabel('MAE')\n",
                "for i, v in enumerate(mae_values):\n",
                "    axes[0, 2].text(i, v + 0.0001, f'{v:.6f}', ha='center')\n",
                "\n",
                "# 4. Predictions vs Actual (first 50 days)\n",
                "n_plot = 50\n",
                "axes[1, 0].plot(y_test_lr.values[:n_plot], label='Actual', color='black', linewidth=2, alpha=0.7)\n",
                "axes[1, 0].plot(ridge_results['predictions'][:n_plot], label='Ridge', alpha=0.7)\n",
                "axes[1, 0].plot(arima_results['predictions'][:n_plot], label='ARIMA', alpha=0.7)\n",
                "axes[1, 0].plot(lstm_results['predictions'][:n_plot], label='LSTM', alpha=0.7)\n",
                "axes[1, 0].set_xlabel('Test Days')\n",
                "axes[1, 0].set_ylabel('Return')\n",
                "axes[1, 0].set_title('Predictions Comparison (First 50 Days)')\n",
                "axes[1, 0].legend()\n",
                "\n",
                "# 5. Error distribution\n",
                "errors = {\n",
                "    'Ridge': ridge_results['predictions'] - y_test_lr.values[:len(ridge_results['predictions'])],\n",
                "    'ARIMA': arima_results['predictions'] - y_test_arima[:len(arima_results['predictions'])],\n",
                "    'LSTM': lstm_results['predictions'] - y_test_seq[:len(lstm_results['predictions'])]\n",
                "}\n",
                "\n",
                "for model_name, error_vals in errors.items():\n",
                "    axes[1, 1].hist(error_vals, bins=30, alpha=0.5, label=model_name, density=True)\n",
                "\n",
                "axes[1, 1].axvline(x=0, color='black', linestyle='--')\n",
                "axes[1, 1].set_xlabel('Prediction Error')\n",
                "axes[1, 1].set_ylabel('Density')\n",
                "axes[1, 1].set_title('Error Distribution')\n",
                "axes[1, 1].legend()\n",
                "\n",
                "# 6. Cumulative returns\n",
                "axes[1, 2].axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
                "\n",
                "for model_name, results in all_results.items():\n",
                "    if model_name == 'Ridge':\n",
                "        preds = results['predictions']\n",
                "        actuals = y_test_lr.values[:len(preds)]\n",
                "    elif model_name == 'ARIMA':\n",
                "        preds = results['predictions']\n",
                "        actuals = y_test_arima[:len(preds)]\n",
                "    else:  # LSTM\n",
                "        preds = results['predictions']\n",
                "        actuals = y_test_seq[:len(preds)]\n",
                "    \n",
                "    positions = np.where(preds > 0, 1, -1)\n",
                "    daily_returns = positions * actuals\n",
                "    cumulative_returns = (1 + daily_returns).cumprod()\n",
                "    \n",
                "    axes[1, 2].plot(cumulative_returns[:100], label=model_name, alpha=0.7)\n",
                "\n",
                "axes[1, 2].set_xlabel('Test Days')\n",
                "axes[1, 2].set_ylabel('Cumulative Return')\n",
                "axes[1, 2].set_title('Cumulative Returns (First 100 Days)')\n",
                "axes[1, 2].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ae499a83",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"TRAINING COMPLETE\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nAll models have been trained and logged to MLflow.\")\n",
                "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"1. Check MLflow UI to see all logged runs\")\n",
                "print(\"2. Run 04_evaluate_select.py to register and compare models\")\n",
                "print(\"3. Deploy the best model for predictions\")\n",
                "\n",
                "# MLflow run info\n",
                "client = mlflow.tracking.MlflowClient()\n",
                "experiment = client.get_experiment_by_name(EXPERIMENT_NAME)\n",
                "if experiment:\n",
                "    runs = client.search_runs(\n",
                "        experiment_ids=[experiment.experiment_id],\n",
                "        order_by=[\"attribute.start_time DESC\"]\n",
                "    )\n",
                "    print(f\"\\nLatest runs in experiment:\")\n",
                "    for i, run in enumerate(runs[:3]):  # Show 3 most recent\n",
                "        model_type = run.data.tags.get('model_type', 'Unknown')\n",
                "        rmse = run.data.metrics.get('rmse', 0)\n",
                "        da = run.data.metrics.get('directional_accuracy', 0)\n",
                "        print(f\"  {i+1}. {model_type}: RMSE={rmse:.6f}, DA={da:.2%}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv_eurusd",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
